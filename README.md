# Bomberman_RL

Final exercise for MLE course SS2023, design of Reinforcement learning algorithm to win Bomberman tournament.

Masking of invalid actions may be allowed, ask! maybe filter out impossible bombs?
It is more important that you come up with experimental techniques to actually identify your best agent for the final submission than have two completely different strategies, 

# Approaches
## Symmetries: 
leveraging symmetries of the game by flipping the board in 3 dimensions (up/down, left/right, diagonal)
It is implemented as a normalization function and is tested in test.py. The game state is a dictionary that gets changed by the function in-place. This year the normalized game state was also passed to the agents via the game_event_ocurred function and the end_of_round function, only one-time normalization during the act() function was necessary. No problem with this function detected yet. Only 'field', 'self', and 'others' were tested but seemed to be sufficient since these covered all implemented internal functions.

![Different symmetries that all represent the same game state](https://github.com/lukevoss/Bomberman_RL/blob/main/symmetry.png)
Different symmetries that all represent the same game state

## Feature extraction:
For feature extraction We chose a deep learning approach, where we use the entire normalized game state as input vector. The function state_to_features converts the game state to a feature vector of size 15x15x7. The walls of the board are not represented to reduce dimensionality
The 7 feature bits (boolean) are represented in the following logic:

| Bit Position  | Feature |
| ------------- | ------------- |
| 0  | Wall  |
| 1  | Crate  |
| 2  | Agent  |
| 3  | Opponent  |
| 4  | Bomb  |
| 5  | Coin  |
| 6  | Explosion  |


In the final step the vector is flattened to get a feature vector for the Deep neural network. The feature extraction is not yet tested but doesnt represent a complex function

## Behavioral Cloning
The idea for efficient training and testing of different Algorithms and network structures was to first test how well they performed in a supervised learning task. A dataset was generated by extracting state information and the following actions from an expert player, in this case, the rule-based agent. By training the different network backbones one can see how much of the information can be stored in the architecture and achieve a well-playing student agent, that could further be trained through self-play. 
Unsolved Problems:
All tried Architectures of the ActorCritic network weren't able to closely mimic the agent. The student agent's first few bombs were mostly correct in games but blows himself up quite often. Resulting Agents have still the best gameplay that has been observed so far.
LSTM Actor Critic showed slightly better validation performance during behavioral cloning than the MLP Actor Critic

## Reinforcement Learning Algorithm
For the reinforcement learning algorithm, the Proximal Policy Optimization was chosen. The Algorithm seems to work fine so far and reduces the loss but gets stuck very fast on bad local optima. For the Actor Critic backbone, I tried an LSTM and MLP approach but could not get good results so far. A potential solution for the local optima could be a denser reward structure so the algorithm can more easily converge to the global optimum. Several PPO Hyperparameters have not been changed and tested yet

